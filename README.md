# Next-Word-Prediction-Using-LSTM
# ğŸ“œ Shakespeare Next-K Word Prediction with LSTM

This project trains an **LSTM (Long Short-Term Memory)** neural network to predict the **next `K` words** in a sequence using the **Shakespeare dataset**.  
It also provides an interactive **FastAPI web interface** where users can type a phrase and get:

- **The next single word prediction**
- **The next `K` words prediction** (default: 3)

---

## ğŸ“‚ Project Structure

.
â”œâ”€â”€ website.py # FastAPI backend
â”œâ”€â”€ lstm_model.pth # Trained LSTM model (PyTorch)
â”œâ”€â”€ vector_dict.json # Word â†’ index mapping
â”œâ”€â”€ inverse_dict.json # Index â†’ word mapping
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ templates/
â”‚ â””â”€â”€ index.html # Web interface template
â””â”€â”€ README.md # This file

yaml
Copy
Edit

---

## ğŸ§  Dataset

We use the **Shakespeare text dataset**, cleaned and tokenized into word sequences.  
The dataset is split into training sequences of fixed length, and each sequence is fed to the LSTM to learn word dependencies.

---

## âš™ï¸ Model Architecture

- **Embedding Layer** â€” Converts words (indices) into dense vector embeddings.
- **LSTM Layer(s)** â€” Captures sequential dependencies in text.
- **Fully Connected Layer** â€” Maps LSTM outputs to vocabulary probabilities.
- **Softmax Activation** â€” Produces probability distribution over the next possible words.

**Loss Function:** CrossEntropyLoss  
**Optimizer:** Adam  
**Framework:** PyTorch

---

## ğŸŒ Web Interface (FastAPI)

The project includes a **FastAPI app** to serve predictions via a browser-based UI.

### Features:
- Type in a phrase and submit.
- View the **next word** and **next 3 words** predicted.
- Clean and responsive HTML/CSS design.

---

## ğŸ“¦ Installation & Setup

### 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/yourusername/your-repo-name.git
cd your-repo-name
2ï¸âƒ£ Create Virtual Environment (Optional but Recommended)
bash
Copy
Edit
python -m venv .venv
source .venv/bin/activate      # On macOS/Linux
.venv\Scripts\activate         # On Windows
3ï¸âƒ£ Install Dependencies
bash
Copy
Edit
pip install -r requirements.txt
requirements.txt should contain:

nginx
Copy
Edit
fastapi
uvicorn
torch
jinja2
ğŸš€ Running the Web App
From your project directory:

bash
Copy
Edit
uvicorn website:app --reload
Open your browser at:

cpp
Copy
Edit
http://127.0.0.1:8000
ğŸ”® How Predictions Work
Text Input is split into words.

Each word is converted into its index using vector_dict.json.

The last N words (e.g., last 5 tokens) are fed into the LSTM.

The model outputs a probability distribution for the next word.

The top predicted index is converted back to a word using inverse_dict.json.

For multi-word prediction, the predicted word is appended to the sequence and the process repeats.

ğŸ›  Error Handling
If a word is not found in vector_dict.json, it is replaced with an <UNK> token index (default: 0).

The system gracefully handles unknown inputs without crashing.

ğŸ“œ Example
Input:

css
Copy
Edit
to be or not
Output:

vbnet
Copy
Edit
Next Word: to
Next 3 Words: to be or
ğŸ“ˆ Training Notes
Sequence length: 5

Vocabulary built from Shakespeare dataset

Trained using Adam optimizer

Saved using:

python
Copy
Edit
torch.save(model, "lstm_model.pth")
ğŸ§‘â€ğŸ’» Author
Your Name
ğŸ“§ your.email@example.com
ğŸ”— GitHub Profile

ğŸ“„ License
This project is licensed under the MIT License.

yaml
Copy
Edit

---

You can literally **copy-paste** this into your `README.md` in GitHub and itâ€™s ready.  

Do you want me to also add a **"How to Retrain the Model"** section so people can fine-tune it on new datasets? That could ma